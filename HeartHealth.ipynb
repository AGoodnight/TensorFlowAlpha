{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d08c898-f45f-4058-ae2b-7b1d9006cd3b",
   "metadata": {},
   "source": [
    "### Handling Mixed Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3154e2-ed7f-462c-a533-0bfac45f0aa1",
   "metadata": {},
   "source": [
    "Import our Libraries etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6b2d61a-c6eb-4c05-a431-3b7972c0c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d166d-c04a-4cca-a70b-0045f048527c",
   "metadata": {},
   "source": [
    "Import data and setup a namespace for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45a296-ed17-49d2-aa0a-e39a738d5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'heart_health'           \n",
    "hh = pd.read_csv('./input/Heart_health.csv')\n",
    "\n",
    "# tensorflow is a bit more strict with column strings now\n",
    "hh.columns = [c.replace(\"/\",\"_\") for c in list(hh.columns)]\n",
    "\n",
    "# prefer namespace over extra top level variables\n",
    "dset = SimpleNamespace(feats=None,labels=None,tdict=None)\n",
    "dset.feats = hh.copy()\n",
    "dset.labels = dset.feats.pop(\"Heart Attack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af285a39-5fd8-4c80-8b39-46e6365a9a5d",
   "metadata": {},
   "source": [
    "To build the preprocessing model, start by building a set of symbolic tf.keras.Input objects, matching the names and data-types of the CSV columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491468e8-1d1b-42cc-8586-45fd8c9cae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_inputs(*,items):\n",
    "    inputs = {}\n",
    "    for name,column in items:\n",
    "        if column.dtype == object:\n",
    "            inputs[name] = tf.keras.Input(shape=(1,),name=name,dtype=tf.string)\n",
    "        else:\n",
    "            inputs[name] = tf.keras.Input(shape=(1,),name=name,dtype=tf.float32)    \n",
    "    \n",
    "    return inputs\n",
    "\n",
    "ktinputs = as_keras_inputs(items=dset.feats.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df7c4d-3bd4-49b2-9903-43c00799b31d",
   "metadata": {},
   "source": [
    "The first step in your preprocessing logic is to concatenate the numeric inputs together, and run them through a normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bed7f-a284-4199-8327-3584b0a665bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_inputs(*,inputs,dataframe):\n",
    "    normalizer = layers.Normalization()\n",
    "    \n",
    "    all_numeric_inputs = {name:input for name,input in inputs.items() if input.dtype==tf.float32}\n",
    "    x = layers.Concatenate()(list(all_numeric_inputs.values()))\n",
    "    normalizer.adapt(np.array(dataframe[all_numeric_inputs.keys()]))\n",
    "    \n",
    "    return normalizer(x)\n",
    "\n",
    "ppinputs = [normalize_numeric_inputs(inputs=ktinputs,dataframe=hh)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9304ce-4ba8-4953-a5ca-f4d82014478c",
   "metadata": {},
   "source": [
    "Now we need to convert our strings into float32 appropriate for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff50cb4-5912-4117-ba03-7518d65f35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_float32(*,inputs,feats):\n",
    "    for name, input in inputs.items():\n",
    "        if input.dtype == tf.float32:\n",
    "            continue\n",
    "\n",
    "        # For the string inputs use the tf.keras.layers.StringLookup function \n",
    "        # to map from strings to integer indices in a vocabulary. \n",
    "        lookup = layers.StringLookup(vocabulary=np.unique(feats[name]))\n",
    "\n",
    "        # Next, use tf.keras.layers.CategoryEncoding to convert the indexes \n",
    "        # into float32 data appropriate for the model.\n",
    "        # The default settings for the tf.keras.layers.CategoryEncoding layer \n",
    "        # create a one-hot vector for each input.\n",
    "        one_hot_vector = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "        \n",
    "        return one_hot_vector(lookup(input))\n",
    "\n",
    "ppinputs.append(strings_to_float32(inputs=ktinputs,feats=dset.feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4994d-eb5c-4d33-9c7a-f8b50730aa1b",
   "metadata": {},
   "source": [
    "With the collection of inputs and preprocessed_inputs, you can concatenate all the preprocessed inputs together, and build a model that handles the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3ecc8-2f2a-4369-a81d-802d148e1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_inputs = layers.Concatenate()(ppinputs)\n",
    "dset_preprocessing_model = tf.keras.Model(inputs=ktinputs,outputs=preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422373e9-ecbb-4a26-912d-4b1b21e651a8",
   "metadata": {},
   "source": [
    "Keras models don't automatically convert pandas DataFrames because it's not clear if it should be converted to one tensor or to a dictionary of tensors. \n",
    "So, convert it to a dictionary of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d57cb9-08be-4402-8704-1ff387261822",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.tdict = {name:np.array(value) for name, value in dset.feats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c25021-100b-47e3-8888-2d92762da810",
   "metadata": {},
   "source": [
    "Now build a model to train based on a sequential model from our \n",
    "preprocessing model and our keras inputs based on the panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31cae659-39ed-46a0-8284-99bc533ec111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.6481\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.2762\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.1563\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.1005\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.0659\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.0500\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - loss: 0.0436\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - loss: 0.0303\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - loss: 0.0245\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.0222\n"
     ]
    }
   ],
   "source": [
    "def build_model(*,preprocessing_head,inputs):\n",
    "    seq_model = tf.keras.Sequential([\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    result = seq_model(preprocessing_head(inputs))\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=result,name=model_name)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer = tf.keras.optimizers.Adam())\n",
    "    return model\n",
    "    \n",
    "dset_model = build_model(preprocessing_head=dset_preprocessing_model,inputs=ktinputs)\n",
    "dset_model.fit(\n",
    "    x=dset.tdict,\n",
    "    y=dset.labels,\n",
    "    epochs=10)\n",
    "\n",
    "dset_model.save(f'{dset_model.name}_test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1efa3-5ea3-4a49-a3ce-a66aff3090c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

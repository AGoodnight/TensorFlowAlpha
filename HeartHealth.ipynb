{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d08c898-f45f-4058-ae2b-7b1d9006cd3b",
   "metadata": {},
   "source": [
    "### Handling Mixed Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb35eddc-a959-4a02-be67-653cb227eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.5291\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.2330\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.1255\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.0919\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.0627\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.0437\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 0.0334\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - loss: 0.0293\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.0258\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - loss: 0.0188\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "def as_keras_inputs(*,items):\n",
    "    inputs = {}\n",
    "    for name,column in items:\n",
    "        if column.dtype == object:\n",
    "            inputs[name] = tf.keras.Input(shape=(1,),name=name,dtype=tf.string)\n",
    "        else:\n",
    "            inputs[name] = tf.keras.Input(shape=(1,),name=name,dtype=tf.float32)    \n",
    "    \n",
    "    return inputs\n",
    "\n",
    "\n",
    "def normalize_numeric_inputs(*,inputs,dataframe):\n",
    "    normalizer = layers.Normalization()\n",
    "    \n",
    "    all_numeric_inputs = {name:input for name,input in inputs.items() if input.dtype==tf.float32}\n",
    "    x = layers.Concatenate()(list(all_numeric_inputs.values()))\n",
    "    normalizer.adapt(np.array(dataframe[all_numeric_inputs.keys()]))\n",
    "    \n",
    "    return normalizer(x)\n",
    "\n",
    "def strings_to_float32(*,inputs,feats):\n",
    "    for name, input in inputs.items():\n",
    "        if input.dtype == tf.float32:\n",
    "            continue\n",
    "\n",
    "        # For the string inputs use the tf.keras.layers.StringLookup function \n",
    "        # to map from strings to integer indices in a vocabulary. \n",
    "        lookup = layers.StringLookup(vocabulary=np.unique(feats[name]))\n",
    "\n",
    "        # Next, use tf.keras.layers.CategoryEncoding to convert the indexes \n",
    "        # into float32 data appropriate for the model.\n",
    "        # The default settings for the tf.keras.layers.CategoryEncoding layer \n",
    "        # create a one-hot vector for each input.\n",
    "        one_hot_vector = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "        \n",
    "        return one_hot_vector(lookup(input))\n",
    "\n",
    "# Build a model based on a sequential model from our preprocessing model \n",
    "# and our keras inputs based on the panda dataframe\n",
    "def build_model(*,preprocessing_head,inputs):\n",
    "    seq_model = tf.keras.Sequential([\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    result = seq_model(preprocessing_head(inputs))\n",
    "    model = tf.keras.Model(inputs,result)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer = tf.keras.optimizers.Adam())\n",
    "    return model\n",
    "    \n",
    "model_name = 'heart_health'           \n",
    "hh = pd.read_csv('./input/Heart_health.csv')\n",
    "# tensorflow is a bit more strict with column strings now\n",
    "hh.columns = [c.replace(\"/\",\"_\") for c in list(hh.columns)]\n",
    "\n",
    "# prefer namespace over extra top level variables\n",
    "dset = SimpleNamespace(feats=None,labels=None,tdict=None)\n",
    "dset.feats = hh.copy()\n",
    "dset.labels = dset.feats.pop(\"Heart Attack\")\n",
    "\n",
    "# To build the preprocessing model, start by building a set of symbolic \n",
    "# tf.keras.Input objects, matching the names and data-types of the CSV columns.\n",
    "ktinputs = as_keras_inputs(items=dset.feats.items())\n",
    "\n",
    "# The first step in your preprocessing logic is to concatenate the \n",
    "# numeric inputs together, and run them through a normalization layer:\n",
    "ppinputs = [normalize_numeric_inputs(inputs=ktinputs,dataframe=hh)]\n",
    "\n",
    "# Now we need to convert our strings into float32 appropriate for the model\n",
    "ppinputs.append(strings_to_float32(inputs=ktinputs,feats=dset.feats))\n",
    "\n",
    "# With the collection of inputs and preprocessed_inputs, you can concatenate all \n",
    "# the preprocessed inputs together, and build a model that handles the preprocessing:\n",
    "\n",
    "preprocessed_inputs = layers.Concatenate()(ppinputs)\n",
    "dset_preprocessing_model = tf.keras.Model(inputs=ktinputs,outputs=preprocessed_inputs,name=model_name)\n",
    "\n",
    "# Keras models don't automatically convert pandas DataFrames because it's not clear if \n",
    "# it should be converted to one tensor or to a dictionary of tensors. \n",
    "# So, convert it to a dictionary of tensors:\n",
    "dset.tdict = {name:np.array(value) for name, value in dset.feats.items()}\n",
    "\n",
    "# Now build a model to train\n",
    "dset_model = build_model(preprocessing_head=dset_preprocessing_model,inputs=ktinputs)\n",
    "dset_model.fit(\n",
    "    x=dset.tdict,\n",
    "    y=dset.labels,\n",
    "    epochs=10)\n",
    "\n",
    "dset_model.save(f'{model_name}_test.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3794af-cb06-41af-8bcf-377f687788dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f0c97-87a9-4b58-9505-c0ff7017e864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
